---
title: Whisper ASR Guide
slug: dHQJ-title-whisper-asr-guide-slug-6ou-w-createdat-2025-04-07t094158045z-updatedat-2025-04-17t032758381z-whisper-is-a-general-purpose-speech-recognition-model-trained-on-a-large-dat
createdAt: Thu Apr 17 2025 03:27:50 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu Apr 17 2025 03:30:55 GMT+0000 (Coordinated Universal Time)
---

**Whisper** is a general-purpose speech recognition model trained on a large dataset of diverse audio. Go through the [Readme](https://cloud.vast.ai/template/readme/0c0c7d65cd4ebb2b340fbce39879703b) first before using.&#x20;

**Connecting to the Instance**


1. Go to the templates tab and search for “*Whisper*” or click the provided link to the template [here](https://cloud.vast.ai/?ref_id=62897\&creator_id=62897\&name=Whisper%20ASR%20Webservice) .&#x20;
2. After you select the template by pressing the triangle button the next step is to choose a gpu.

![](/images/use-cases-audio-to-text.png)

3\. **Select a GPU Offering&#x20;**

![](/images/use-cases-audio-to-text-2.png)

The template you selected will give your instance access to both Jupyter and SSH. Additionally the Open button will connect you to the instance portal web interface.&#x20;

4\. HTTP and token-based auth are both enabled by default. To avoid certificate errors in your browser, please follow the instructions for installing the TLS certificate [here](/documentation/instances/jupyter#1SmCz) to allow secure HTTPS connections to your instance via its IP.&#x20;

![](/images/use-cases-audio-to-text-3.png)

5\. Use the open button to open up the instance, if you are not using the open button the default username will be: vastai , and the password will be the value of the environment variable:*&#x20;OPEN\_BUTTON\_TOKEN*. You can also find the token value by accessing the terminal and executing this command: *echo $OPEN\_BUTTON\_TOKEN*

![](/images/use-cases-audio-to-text-4.png)

6\. After accessing the SwaggerUi by clicking the triangle button first then waiting for the page to load, then clicking into the link aligning with SwaggerUI you should see the page below. (note: usually loads fast but can take 5-10 minutes)&#x20;


![](/images/use-cases-audio-to-text-5.png)

**Usage**

Two POST endpoints are exposed in this template:

**/detect-language**

Use this endpoint to automatically detect the spoken language in a given audio file.

![](/images/use-cases-audio-to-text-6.png)

**/asr**

Use this endpoint for both transcription and translation of audio files.

*Both of these endpoints are documented using the OpenAPI standard and can be tested in a web browser.&#x20;*

![](/images/use-cases-audio-to-text-7.png)

7\. *Select the detect language endpoint*

8\. *Then click try it out.&#x20;*

![](/images/use-cases-audio-to-text-8.png)

9.*&#x20;From here upload an audio clip*&#x20;

10\. *Then press the execute button.&#x20;*

![](/images/use-cases-audio-to-text-9.png)

11.*&#x20;If you look in the response body (see below) you can see it was able to detect the language was English.*&#x20;

*Note: If you are getting an internal 500 error its most likely the file you selected to upload is to large.&#x20;*

![](/images/use-cases-audio-to-text-10.png)

*For more information and specifics on things such as but not limited to Configuration, Additional Functionality, Instance Logs, Cloudflared, Api request, ssh tunnels and port reference mapping, and Caddy you can visit the*[ Readme linked here to learn more. ](https://cloud.vast.ai/template/readme/0c0c7d65cd4ebb2b340fbce39879703b)


**Links**

- [GitHub Repository](https://github.com/ahmetoner/whisper-asr-webservice/)
- [Docker Image](https://hub.docker.com/r/onerahmet/openai-whisper-asr-webservice)

